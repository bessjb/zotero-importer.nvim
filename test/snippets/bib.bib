@journalArticle{yanSECONDSparselyEmbedded2018,
  pages = {3337-},
  abstractNote = {LiDAR-based or RGB-D-based object detection is used in numerous applications, ranging from autonomous driving to robot vision. Voxel-based 3D convolutional networks have been used for some time to enhance the retention of information when processing point cloud LiDAR data. However, problems remain, including a slow inference speed and low orientation estimation performance. We therefore investigate an improved sparse convolution method for such networks, which significantly increases the speed of both training and inference. We also introduce a new form of angle loss regression to improve the orientation estimation performance and a new data augmentation approach that can enhance the convergence speed and performance. The proposed network produces state-of-the-art results on the KITTI 3D object detection benchmarks while maintaining a fast inference speed.},
  language = {eng},
  title = {SECOND: Sparsely Embedded Convolutional Detection},
  shortTitle = {SECOND},
  author = {Yan, Yan and Mao, Yuxing and Li, Bo},
  date = {2018-00-00 2018},
  volume = {18},
  libraryCatalog = {qut.primo.exlibrisgroup.com},
  publicationTitle = {Sensors (Basel, Switzerland)},
  key = {F45T2D8X},
  extra = {Place: Switzerland
Publisher: MDPI},
  issue = {10},
  ISSN = {1424-8220},
  DOI = {10.3390/s18103337},
}
@conferencePaper{zhouVoxelNetEndtoEndLearning2018,
  pages = {4490-4499},
  proceedingsTitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  abstractNote = {Accurate detection of objects in 3D point clouds is a central problem in many applications, such as autonomous navigation, housekeeping robots, and augmented/virtual reality. To interface a highly sparse LiDAR point cloud with a region proposal network (RPN), most existing efforts have focused on hand-crafted feature representations, for example, a bird's eye view projection. In this work, we remove the need of manual feature engineering for 3D point clouds and propose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. Specifically, VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding (VFE) layer. In this way, the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections. Experiments on the KITTI car detection benchmark show that VoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a large margin. Furthermore, our network learns an effective discriminative representation of objects with various geometries, leading to encouraging results in 3D detection of pedestrians and cyclists, based on only LiDAR.},
  conferenceName = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title = {VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection},
  shortTitle = {VoxelNet},
  key = {3TI7MGVD},
  date = {2018-06-00 2018-06},
  accessDate = {2025-06-02 02:41:12},
  author = {Zhou, Yin and Tuzel, Oncel},
  extra = {ISSN: 2575-7075},
  DOI = {10.1109/CVPR.2018.00472},
  url = {https://ieeexplore.ieee.org/document/8578570},
  libraryCatalog = {IEEE Xplore},
}
